{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "cpp"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Floating point basics\n",
    "\n",
    "Floating-point numbers can represent a very large range of numbers, from the smallest to the largest, similarly to scientific notation. They are the prefered types for scientific computing. Yet, one must be aware of the many **rounding errors** which are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "cpp"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "When such a large range is not useful, one can use fixed-point numbers to obtain faster calculations. When looking for maximum precision, while accepting a longer execution time, one can rely on quotients of integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "cpp"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General representation of a floating-point number\n",
    "\n",
    "In a given base *b*, which will be the same for all the numbers, the representation of a number includes: \n",
    "* one bit for the sign (equal to −1 or 1),\n",
    "* the mantissa,\n",
    "* the exponent (relative integer).\n",
    "\n",
    "The encoded number is equivalent to *sign × mantissa × b^exponent*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "cpp"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "When the exponent vary, the comma somehow “float”, hence the name. The mantissa is a sequence of digits in base b, generally of fixed size, in which we choose to place a comma at a given position: just before or just after the first digit, or even just after the last digit (in this case the mantissa is a natural number)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "cpp"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Floating-point numbers are a kind of **computer equivalent of scientific notation**, The later rather using a base of 10 and placing the comma after the first digit of the mantissa. Over time several computing variants have emerged, inducing problems of portability. This motivated the creation of the IEEE 754 standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "cpp"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## IEEE 754 standard\n",
    "\n",
    "In 1984 the IEEE (Institute of Electrical and Electronics Engineers) defined its standard * IEEE 754 *, which was improved in 1987, 2008, and recently in July 2019.\n",
    "\n",
    "The standard proposes different formats in base 2 and in base 10. For base 2, it imposes that the comma is placed after the first bit of the mantissa. Since, in base `2`, the first bit is always` 1`, we do not store it: it is \"implicit\". For the rest, the standard distributes the bits as follows:\n",
    "\n",
    "| Name      |         Common name |     Size |  Sign | Exponent |  Mantissa | Significant decimal digits |\n",
    "| :-------- | ------------------- | -------: | ----: | -------: | --------: | -------------------------: |\n",
    "| binary16  |      Half precision |  16 bits | 1 bit |   5 bits |   10 bits |                          ? |\n",
    "| binary32  |    Single precision |  32 bits | 1 bit |   8 bits |   23 bits |                    about 7 |\n",
    "| binary64  |    Double precision |  64 bits | 1 bit |  11 bits |   52 bits |                   about 16 |\n",
    "| binary128 | Quadruple precision | 128 bits | 1 bit |  15 bits |  112 bits |                          ? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "cpp"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predefined floating-point types in C++\n",
    "\n",
    "The standard does not enforce size of floating-point types. This can cause portability issues from one machine to another. The only guarantee that the standard provides us is\n",
    "\n",
    "```\n",
    "sizeof(float) <= sizeof(double) <= sizeof(long double)\n",
    "```\n",
    "\n",
    "However, nowadays, we can assume that all the architectures use the format **IEEE 754 binary32 for the type `float`**, and the format **IEEE 754 binary64 for the type` double`** and it should come with an efficient hardware implementation for operations on these numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "cpp"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The situation varies more widely for `long double`. If the size of the type on your architecture is 16 bytes (128 bits), it is common that only the first 80 bits are used for the computation, at least for an Intel processor.\n",
    "\n",
    "It is also possible that some operations are not directly supported by the hardware, but rather \"software-emulated\", with reduced performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "cpp"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To check for possible portability issues, you can add `static_assert` into your code in order to verify that the size of the types is what you expect.\n",
    "\n",
    "If you restrain yourself to the g++ compiler, and accept its extensions to the standard, g++ offers you either `__float80` or` __float128` types, depending on the architecture you are compiling on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "cpp"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exotic precisions\n",
    "\n",
    "If the level of precision of `long double` is not enough for you, there are few ways to explore (below). Of course, as there is no direct hardware support, the execution times may be a lot longer.\n",
    "* the GNU library [**MPFR**](https://fr.wikipedia.org/wiki/GNU_MPFR) allows one to choose an arbitrary precision.\n",
    "* of course, as for many other problems, there is the [Boost library](https://www.boost.org/doc/libs/1_71_0/libs/multiprecision/doc/html/boost_multiprecision/intro.html) :)\n",
    "* some algorithms of **compensated arithmetic** make it possible to limit the losses linked to rounding during a calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "cpp"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In addition, especially since the great comeback of machine learning, there is a growing need for numbers with lower precision:\n",
    "* The **half-precision**, which is stored in 2 bytes, will also rely on an external library and will only be effective on certain specific hardware such as GPGPUs;\n",
    "* **BF16** is a variant of the previous one which uses more bits for the exponent and less for the mantissa and which therefore favors \"order of magnitude\" to the detriment of precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "cpp"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It should also be mentioned that the **80-bit extended precision** available on Intel processors was used in the intermediate calculations even though the start and end data were `double`. On the contrary, it was not used for vectorized instructions, which led to different results depending on whether a code was vectorized or not. As far as we know, compilers now avoid this use of 80 bits, even for scalar computations, unless explicitly asked for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New in C++23 : fixed width floating-point types\n",
    "\n",
    "***implementation dependant***\n",
    "\n",
    "If the implementation supports any of the following ISO 60559 types as an extended floating-point type, then:\n",
    "- the corresponding macro is defined as 1 to indicate support,\n",
    "- the corresponding floating-point literal suffix is available, and\n",
    "- the corresponding type alias name is provided:\n",
    "\n",
    "| Type name Defined in header <stdfloat> | Literal suffix | Predefined macro | bits of storage | bits of precision | bits of exponent | max exponent |\n",
    "|:----------------|:-------------|:----------------------|:----|:----|:---|:------|\n",
    "| std::float16_t  | f16 or F16   | __STDCPP_FLOAT16_T__  | 16  | 11  | 5  | 15    |\n",
    "| std::float32_t  | f32 or F32   | __STDCPP_FLOAT32_T__  | 32  | 24  | 8  | 127   |\n",
    "| std::float64_t  | f64 or F64   | __STDCPP_FLOAT64_T__  | 64  | 53  | 11 | 1023  |\n",
    "| std::float128_t | f128 or F128 | __STDCPP_FLOAT128_T__ | 128 | 113 | 15 | 16383 |\n",
    "| std::bfloat16_t | bf16 or BF16 | __STDCPP_BFLOAT16_T__ | 16  | 8   | 8  | 127   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "cpp"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Using the numeric traits defined via [std::numeric_limits](https://en.cppreference.com/w/cpp/types/numeric_limits) in the standard library, complete the following table:\n",
    "\n",
    "| Type        | Size (in bytes)      | Minimal value        | Maximum value        |  Epsilon             |\n",
    "| :---------- | -------------------: | -------------------: | -------------------: | -------------------: |\n",
    "| float       |                      |                      |                      |                      |\n",
    "| double      |                      |                      |                      |                      |\n",
    "| long double |                      |                      |                      |                      |\n",
    "\n",
    "To make sure you don't lose any information while printing out, make sure you display 18 significant digits.\n",
    "* What do we mean here by \"minimal value\" ? What is the difference with the \"lowest value\" ?\n",
    "* What do you think of the results for `long double`: it is rather 80 or 128 bits ?\n",
    "\n",
    "<!--\n",
    "A propos du long double, 80 ou 128 bits ? A priori le 80 bits utilise\n",
    "autant de bits que le 128 bits pour l'exposant, donc le min/max n'est\n",
    "pas discrimant. Par contre, en 128 bits, il me semble que le nombre de\n",
    "chiffres significatif doit etre supérieur à 30, et donc l'epsilon devrait\n",
    "être inférieur à 1e-30, sans quoi c'est sans doute du 80 bits...\n",
    "même si la taille apparente est dans tous les cas de 16 octets.\n",
    "-->\n",
    "\n",
    "If your compiler support it, try the same with the new  fixed width floating-point types:\n",
    "\n",
    "| Type            | Size (in bytes) | Minimal value        | Maximum value        |  Epsilon             |\n",
    "| :-------------- | --------------: | -------------------: | -------------------: | -------------------: |\n",
    "| std::float16_t  |                 |                      |                      |                      |\n",
    "| std::bfloat16_t |                 |                      |                      |                      |\n",
    "| std::float32_t  |                 |                      |                      |                      |\n",
    "| std::float64_t  |                 |                      |                      |                      |\n",
    "| std::float128_t |                 |                      |                      |                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Sources\n",
    "\n",
    "* [LearnCpp](https://www.learncpp.com/cpp-tutorial/floating-point-numbers/)\n",
    "* [Interflop](https://www.interflop.fr/)\n",
    "* [Wikipedia](https://en.wikipedia.org/wiki/Floating-point_arithmetic)\n",
    "* [Cpp Reference](https://en.cppreference.com/w/cpp/language/types)\n",
    "* [Fixed width floating-point types](https://en.cppreference.com/w/cpp/types/floating-point)\n",
    "* [QuadMath](https://people.sc.fsu.edu/~jburkardt/cpp_src/g++_quadmath/g++_quadmath.html)\n",
    "* [What Every Programmer Should Know About Floating-Point Arithmetic](https://floating-point-gui.de/)\n",
    "* [IEEE-754 Floating-Point Conversion](https://babbage.cs.qc.cuny.edu/IEEE-754.old/Decimal.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "© *CNRS 2024*  \n",
    "*This document was created by David Chamont and translated by Olga Abramkina. It is available under the [License Creative Commons - Attribution - No commercial use - Shared under the conditions 4.0 International](http://creativecommons.org/licenses/by-nc-sa/4.0/)*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++17",
   "name": "xcpp17"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
